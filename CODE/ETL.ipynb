{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E T L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DESCOMPRESIÓN Y PREPARACIÓN DE LOS ARCHIVOS JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from textblob import TextBlob\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame combinado tiene 120445 filas.\n",
      "Primeras filas del DataFrame combinado:\n",
      "  publisher genres app_name title  url release_date tags reviews_url specs  \\\n",
      "0       NaN    NaN      NaN   NaN  NaN          NaN  NaN         NaN   NaN   \n",
      "1       NaN    NaN      NaN   NaN  NaN          NaN  NaN         NaN   NaN   \n",
      "2       NaN    NaN      NaN   NaN  NaN          NaN  NaN         NaN   NaN   \n",
      "3       NaN    NaN      NaN   NaN  NaN          NaN  NaN         NaN   NaN   \n",
      "4       NaN    NaN      NaN   NaN  NaN          NaN  NaN         NaN   NaN   \n",
      "\n",
      "  price early_access   id developer  \n",
      "0   NaN          NaN  NaN       NaN  \n",
      "1   NaN          NaN  NaN       NaN  \n",
      "2   NaN          NaN  NaN       NaN  \n",
      "3   NaN          NaN  NaN       NaN  \n",
      "4   NaN          NaN  NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "'''DESCOMPRIME, LEE EL ARCHIVO  Y CONVIERTE EL ARCHIVO steam_games.json.gz A UN ARCHIVO CSV'''\n",
    "# Ruta al archivo comprimido\n",
    "steam_games_json = r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\steam_games.json.gz'\n",
    "\n",
    "# Lista para almacenar los DataFrames de cada objeto JSON\n",
    "df_steam_games = []\n",
    "\n",
    "# Descomprimir y leer el archivo JSON\n",
    "with gzip.open(steam_games_json, 'rt', encoding='utf-8') as file:\n",
    "    # Leer todo el contenido del archivo como una sola cadena\n",
    "    content = file.read()\n",
    "    # Dividir la cadena en líneas, cada línea es un objeto JSON\n",
    "    json_objects = content.strip().split('\\n')\n",
    "    # Procesar y aplanar cada objeto JSON\n",
    "    for json_str in json_objects:\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            df = pd.json_normalize(data)\n",
    "            df_steam_games.append(df)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "if df_steam_games:\n",
    "    raw_games_df = pd.concat(df_steam_games, ignore_index=True)\n",
    "    print(f\"El DataFrame combinado tiene {len(raw_games_df)} filas.\")\n",
    "    print(\"Primeras filas del DataFrame combinado:\")\n",
    "    print(raw_games_df.head())\n",
    "else:\n",
    "    print(\"No se encontraron datos para cargar en DataFrames.\")\n",
    "\n",
    "# Guarda el dataframe en un archivo csv      \n",
    "#raw_games_df.to_csv(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\steam_games.json\\output_steam_games.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame combinado tiene 25799 filas.\n",
      "Primeras filas del DataFrame combinado:\n",
      "             user_id                                           user_url  \\\n",
      "0  76561197970982479  http://steamcommunity.com/profiles/76561197970...   \n",
      "1            js41637               http://steamcommunity.com/id/js41637   \n",
      "2          evcentric             http://steamcommunity.com/id/evcentric   \n",
      "3              doctr                 http://steamcommunity.com/id/doctr   \n",
      "4          maplemage             http://steamcommunity.com/id/maplemage   \n",
      "\n",
      "                                             reviews  \n",
      "0  [{'funny': '', 'posted': 'Posted November 5, 2...  \n",
      "1  [{'funny': '', 'posted': 'Posted June 24, 2014...  \n",
      "2  [{'funny': '', 'posted': 'Posted February 3.',...  \n",
      "3  [{'funny': '', 'posted': 'Posted October 14, 2...  \n",
      "4  [{'funny': '3 people found this review funny',...  \n"
     ]
    }
   ],
   "source": [
    "'''DESCOMPRIME, LEE EL ARCHIVO  Y CONVIERTE EL ARCHIVO user_reviews.json.gz A UN ARCHIVO CSV'''\n",
    "\n",
    "# Ruta al archivo comprimido\n",
    "user_reviews_json = r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\user_reviews.json.gz'\n",
    "df_user_reviews = []\n",
    "\n",
    "def escape_quotes_in_reviews(line):\n",
    "    # Esta función utiliza una expresión regular para escapar comillas dobles dentro de los valores del campo \"review\"\n",
    "    # Asegura que solo se modifican las comillas dentro de los valores y no las que delimitan los campos JSON\n",
    "    pattern = r'(?<=review\": \")(.*?)(?=\"})'  # Captura el contenido dentro del campo \"review\"\n",
    "    def replace(match):\n",
    "        # Escapar todas las comillas dobles dentro del texto del review\n",
    "        escaped_text = match.group(0).replace('\"', '**')\n",
    "        return escaped_text\n",
    "    return re.sub(pattern, replace, line, flags=re.DOTALL)\n",
    "\n",
    "with gzip.open(user_reviews_json, 'rt', encoding='utf-8') as file:\n",
    "    line_num = 1\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Modifica cada línea para que obtenga el formato json necesario. Se hace un uso adecuado de comillas, escapes, booleanos, etc.\n",
    "            line = line.replace(\"\\\"\", '**')  \n",
    "            line = line.replace(\"**}\", '\"}')  \n",
    "            line = line.replace(': **', ': \"')  \n",
    "            line = line.replace(\"',\", '\",')  \n",
    "            line = line.replace(\", '\", ', \"')\n",
    "            line = line.replace(\"':\", '\":') \n",
    "            line = line.replace(\": '\", ': \"')  \n",
    "            line = line.replace(\"'}\", '\"}')  \n",
    "            line = line.replace(\"{'\", '{\"')  \n",
    "            line = line.replace('\\\\', '')  \n",
    "            line = line.replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "            corrected_line = escape_quotes_in_reviews(line)\n",
    "            \n",
    "            # Carga el json corregido a \"data\" si todo salió bien en la parte previa, de lo contrario avisa que hubo un error e imprime la línea problemática\n",
    "            if corrected_line:\n",
    "                #print(corrected_line)\n",
    "                data = json.loads(corrected_line)\n",
    "                df = pd.json_normalize(data)  # Normalizar y convertir a DataFrame\n",
    "                df_user_reviews.append(df)\n",
    "                #print(line)\n",
    "            else:\n",
    "                print(\"Error al procesar la línea:\", line)\n",
    "        \n",
    "        # Si encuentra errores, imprime el error y la línea problemática.        \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error en la línea {line_num}: {str(e)}\")\n",
    "            print(\"Error al procesar la línea:\", line)\n",
    "            \n",
    "        line_num += 1\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "if df_user_reviews:\n",
    "    raw_reviews_df = pd.concat(df_user_reviews, ignore_index=True)\n",
    "    print(f\"El DataFrame combinado tiene {len(raw_reviews_df)} filas.\")\n",
    "    print(\"Primeras filas del DataFrame combinado:\")\n",
    "    print(raw_reviews_df.head())\n",
    "else:\n",
    "    print(\"No se encontraron datos para cargar en DataFrames.\")\n",
    "\n",
    "# Guarda el dataframe en un archivo csv      \n",
    "#raw_reviews_df.to_csv(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\user_reviews.json\\australian_user_reviews.csv', index=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame combinado tiene 88310 filas.\n",
      "Primeras filas del DataFrame combinado:\n",
      "             user_id  items_count           steam_id  \\\n",
      "0  76561197970982479          277  76561197970982479   \n",
      "1            js41637          888  76561198035864385   \n",
      "2          evcentric          137  76561198007712555   \n",
      "3         Riot-Punch          328  76561197963445855   \n",
      "4              doctr          541  76561198002099482   \n",
      "\n",
      "                                            user_url  \\\n",
      "0  http://steamcommunity.com/profiles/76561197970...   \n",
      "1               http://steamcommunity.com/id/js41637   \n",
      "2             http://steamcommunity.com/id/evcentric   \n",
      "3            http://steamcommunity.com/id/Riot-Punch   \n",
      "4                 http://steamcommunity.com/id/doctr   \n",
      "\n",
      "                                               items  \n",
      "0  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
      "1  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
      "2  [{'item_id': '1200', 'item_name': 'Red Orchest...  \n",
      "3  [{'item_id': '10', 'item_name': 'Counter-Strik...  \n",
      "4  [{'item_id': '300', 'item_name': 'Day of Defea...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''DESCOMPRIME, LEE EL ARCHIVO  Y CONVIERTE EL ARCHIVO users_items.json.gz A UN ARCHIVO CSV - ESTE PROCESO TARDA APROX. 17 MINUTOS, NO ES EFICIENTE PERO FUNCIONA'''\n",
    "\n",
    "user_items_json = r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\users_items.json.gz'\n",
    "\n",
    "\n",
    "df_user_items = []\n",
    "\n",
    "def escape_quotes_in_reviews(line):\n",
    "    # Esta función utiliza una expresión regular para escapar comillas dobles dentro de los valores del campo \"review\"\n",
    "    # Asegura que solo se modifican las comillas dentro de los valores y no las que delimitan los campos JSON\n",
    "    pattern = r'(?<=item_name\": \")(.*?)(?=\"})'  # Captura el contenido dentro del campo \"review\"\n",
    "    def replace(match):\n",
    "        # Escapar todas las comillas dobles dentro del texto del review\n",
    "        escaped_text = match.group(0).replace('\"', '**')\n",
    "        return escaped_text\n",
    "    return re.sub(pattern, replace, line, flags=re.DOTALL)\n",
    "\n",
    "\n",
    "with gzip.open(user_items_json, 'rt', encoding='utf-8') as file:\n",
    "    line_num = 1\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Modifica cada línea para que obtenga el formato json necesario. Se hace un uso adecuado de comillas, escapes, booleanos, etc.\n",
    "            line = line.replace(\"\\\"\", '**')  \n",
    "            line = line.replace(\"**}\", '\"}')  \n",
    "            line = line.replace(': **', ': \"')\n",
    "            line = line.replace('**,', '\",')  \n",
    "            line = line.replace(\"',\", '\",')  \n",
    "            line = line.replace(\", '\", ', \"')\n",
    "            line = line.replace(\"':\", '\":') \n",
    "            line = line.replace(\": '\", ': \"')  \n",
    "            line = line.replace(\"'}\", '\"}')  \n",
    "            line = line.replace(\"{'\", '{\"')  \n",
    "            line = line.replace('\\\\', '')  \n",
    "            line = line.replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "            corrected_line = escape_quotes_in_reviews(line)\n",
    "            \n",
    "            # Carga el json corregido a \"data\" si todo salió bien en la parte previa, de lo contrario avisa que hubo un error e imprime la línea problemática\n",
    "            if corrected_line:\n",
    "                data = json.loads(corrected_line)\n",
    "                df = pd.json_normalize(data)  # Normalizar y convertir a DataFrame\n",
    "                df_user_items.append(df)\n",
    "           \n",
    "            else:\n",
    "                print(\"Error al procesar la línea:\", line)\n",
    "        \n",
    "        # Si encuentra errores, imprime el error y la línea problemática.        \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error en la línea {line_num}: {str(e)}\")\n",
    "            print(\"Error al procesar la línea:\", line)\n",
    "            \n",
    "        line_num += 1\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "if df_user_items:\n",
    "    raw_uitems_df = pd.concat(df_user_items, ignore_index=True)\n",
    "    print(f\"El DataFrame combinado tiene {len(raw_uitems_df)} filas.\")\n",
    "    print(\"Primeras filas del DataFrame combinado:\")\n",
    "    print(raw_uitems_df.head())\n",
    "else:\n",
    "    print(\"No se encontraron datos para cargar en DataFrames.\")\n",
    "  \n",
    "# Guarda el dataframe en un archivo csv  \n",
    "#raw_uitems_df.to_csv(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\users_items.json\\australian_users_items.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EXTRACCIÓN DE LA INFORMACIÓN\n",
    "Lee los archivos csv's generados y crea nuevos que contienen la información estrictamente necesaria para alimentar las funciones que conformarán la API del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Crea el PARQUET que servirá para la función 1: PlaytimeGenre\n",
    "NOTA: La base de datos que se genera a partir de los datos orginales posee un total de 9,993,947 de filas. Por lo tanto, se tomó la decisión de crear una muestra aleatoria con el 1% de los datos con el fin de prevenir problemas relacionados con el uso de memoria. De esta  manera  se logra probar  la correcta ejecución de funciones y deploy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 0. Lee los csv's de cada base de datos #####\n",
    "raw_games_csv = pd.read_csv(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\steam_games.json\\output_steam_games.csv', dtype={0: str, 1: str, 2: str, 3: str, 4: str, 5: str, 6: str, 7: str, 8: str, 9: str, 10: str, 11:object, 12: str})\n",
    "raw_uitems_csv = pd.read_csv(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\users_items.json\\australian_users_items.csv')\n",
    "\n",
    "###### 1. Prepara el csv de raw_uitems_csv ######\n",
    "# Configurar el campo 'items' para que sea interpretado correctamente y lea las listas\n",
    "raw_uitems_csv['items'] = raw_uitems_csv['items'].apply(ast.literal_eval) # Realiza la conversión\n",
    "#print(raw_uitems_csv['items'].apply(type).value_counts())  # Imprime el tipo de dato contenido en el campo 'items' y el número de filas\n",
    "\n",
    "# Explota el dataframe para que se escriba un renglón por lista de cada usuario (se repite el usuario en cada fila)\n",
    "uitems_exploded = raw_uitems_csv.explode('items')\n",
    "# print(len(uitems_exploded)) # Imprime el nuevo número de filas (debe ser mucho mayor al del csv)\n",
    "\n",
    "# Extraer claves de diccionario en nuevas columnas, se excluyen los valores NaN \n",
    "uitems_exploded['item_id'] = uitems_exploded['items'].apply(lambda x: x.get('item_id') if pd.notna(x) else x)\n",
    "uitems_exploded['playtime_forever'] = uitems_exploded['items'].apply(lambda x: x.get('playtime_forever') if pd.notna(x) else x)\n",
    "\n",
    "# Borra todas filas que contienen Nan en la columna de tiempo de juego (playtime_forever)\n",
    "uitems_exploded= uitems_exploded.dropna(subset=['playtime_forever'])\n",
    "# print (uitems_exploded['item_id'].isna().sum()) # R = 16,806 valores nulos\n",
    "# print (uitems_exploded['playtime_forever'].isna().sum()) # R = 16,806 valores nulos\n",
    "\n",
    "##### 2. Prepara el csv de raw_gamses_csv #####\n",
    "# Borra todas filas que contienen solo Nan en todas sus columnas\n",
    "raw_games_csv_clean = raw_games_csv.dropna(how='all')\n",
    "raw_games_csv_clean = raw_games_csv_clean.dropna(subset=['genres'])\n",
    "raw_games_csv_clean = raw_games_csv_clean.dropna(subset=['id'])\n",
    "\n",
    "##### 3. JOIN: user items con steam games (Dataframe para responder función 1) #####\n",
    "# Hacer un join  con las df raw_utiems y raw_games, pero solo conserva las columnas estrictamente necesarias\n",
    "horas_jugadas = pd.merge(uitems_exploded[['item_id', 'playtime_forever']], raw_games_csv_clean[['id', 'release_date', 'genres']], left_on='item_id', right_on='id', how='inner')\n",
    "\n",
    "# Convertir cadenas a  listas\n",
    "horas_jugadas['genres'] = horas_jugadas['genres'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir 'release_date' a formato de fecha (datetime)\n",
    "horas_jugadas['release_date'] = pd.to_datetime(horas_jugadas['release_date'],errors='coerce')\n",
    "\n",
    "# Crea una nueva columna que solo contiene el año\n",
    "horas_jugadas['release_year'] = horas_jugadas['release_date'].dt.year\n",
    "# Conocer el periodo que abarca esta base de datos:\n",
    "# print('Año minimo: ', horas_jugadas['release_year'].min())\n",
    "# print('Año máximo: ', horas_jugadas['release_year'].max())\n",
    "\n",
    "# Cada videojuego puede tener varias categorías  de  género, por lo cual, se expande este campo para contabilizar \n",
    "# por separado cada género. Se deja fuera la etiqueta de Early Access pues no se considera un género.\n",
    "horas_jugadas = horas_jugadas.explode('genres')\n",
    "\n",
    "# Genera una muestra aleatoria con el \n",
    "horas_jugadas = horas_jugadas.sample(frac = 0.01, random_state=42)\n",
    "###------Crea el CSV que servirá para la función 1: PlaytimeGenre-----\n",
    "#horas_jugadas.to_csv(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\F01_PlaytimeGenre.csv', index=False)\n",
    "\n",
    "###------Crea el parquet que servirá para la función 1: PlaytimeGenre-----\n",
    "horas_jugadas.to_parquet(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\F01_PlaytimeGenre.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Crea el PARQUET que servirá para la función 2: UserForGenre\n",
    " NOTA: La base de datos que se genera a partir de los datos orginales posee un total de 9,993,947 de filas. Por lo tanto, se tomó la decisión de crear una muestra aleatoria con el 1% de los datos con el fin de prevenir problemas relacionados con el uso de memoria. De esta  manera  se logra probar  la correcta ejecución de funciones y deploy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 4. JOIN: user items con steam games (Dataframe para responder función 2) #####\n",
    "# Hacer un join  con las df raw_utiems y raw_games, pero solo conserva las columnas estrictamente necesarias\n",
    "horas_jugadas_usuario = pd.merge(uitems_exploded[['user_id','item_id', 'playtime_forever']], raw_games_csv_clean[['id', 'release_date', 'genres']], left_on='item_id', right_on='id', how='inner')\n",
    "\n",
    "# Borra todas filas que contienen Nan en la columna de géneros (genres)\n",
    "horas_jugadas_usuario = horas_jugadas_usuario.dropna(subset=['genres'])\n",
    "\n",
    "# Borra todas filas que contienen Nan en la columna de user_id \n",
    "horas_jugadas_usuario = horas_jugadas_usuario.dropna(subset=['user_id'])\n",
    "\n",
    "# Convertir cadenas a  listas\n",
    "horas_jugadas_usuario['genres'] = horas_jugadas_usuario['genres'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Convertir 'release_date' a formato de fecha (datetime)\n",
    "horas_jugadas_usuario['release_date'] = pd.to_datetime(horas_jugadas_usuario['release_date'],errors='coerce')\n",
    "\n",
    "# Crea una nueva columna que solo contiene el año\n",
    "horas_jugadas_usuario['release_year'] = horas_jugadas_usuario['release_date'].dt.year\n",
    "\n",
    "# Cada videojuego puede tener varias categorías  de  género, por lo cual, se expande este campo para contabilizar \n",
    "# por separado cada género. Se deja fuera la etiqueta de Early Access pues no se considera un género.\n",
    "horas_jugadas_usuario = horas_jugadas_usuario.explode('genres')\n",
    "\n",
    "horas_jugadas_usuario = horas_jugadas_usuario.sample(frac = 0.01, random_state=42)\n",
    "\n",
    "###-----Crea el CSV que servirá para la función 2: UserForGenre----\n",
    "#horas_jugadas_usuario.to_csv(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\F02_UserForGenre.csv', index=False)\n",
    "\n",
    "###-----Crea el parquet que servirá para la función 2: UserForGenre----\n",
    "horas_jugadas_usuario.to_parquet(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\F02_UserForGenre.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Crea el PARQUET que servirá para la función 3: UsersRecommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_reviews_csv = pd.read_csv(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\user_reviews.json\\australian_user_reviews.csv')\n",
    "\n",
    "###### 1. Prepara el csv de raw_reviews_csv ######\n",
    "# Configurar el campo 'reviews' para que sea interpretado correctamente y lea las listas\n",
    "raw_reviews_csv['reviews'] = raw_reviews_csv['reviews'].apply(ast.literal_eval) # Realiza la conversión\n",
    "#print(raw_uitems_csv['items'].apply(type).value_counts())  # Imprime el tipo de dato contenido en el campo 'items' y el número de filas\n",
    "\n",
    "# Explota el dataframe para que se escriba un renglón por lista de cada usuario (se repite el usuario en cada fila)\n",
    "reviews_exploded = raw_reviews_csv.explode('reviews')\n",
    "# print(len(uitems_exploded)) # Imprime el nuevo número de filas (debe ser mucho mayor al del csv)\n",
    "\n",
    "# Extraer claves de diccionario en nuevas columnas, se excluyen los valores NaN \n",
    "reviews_exploded['review'] = reviews_exploded['reviews'].apply(lambda x: x.get('review') if pd.notna(x) else x)\n",
    "reviews_exploded['item_id'] = reviews_exploded['reviews'].apply(lambda x: x.get('item_id') if pd.notna(x) else x)\n",
    "reviews_exploded['recommend'] = reviews_exploded['reviews'].apply(lambda x: x.get('recommend') if pd.notna(x) else x)\n",
    "\n",
    "# Genera una función para hacer el análisis de sentimiento y crear una nueva columna con el resultado del análisis\n",
    "def analyze_sentiment(df):\n",
    "    \"\"\"\n",
    "    Analiza el sentimiento de las reseñas en un DataFrame y agrega una nueva columna con el resultado.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pandas.DataFrame): DataFrame que contiene las columnas de reseñas.\n",
    "    \n",
    "    Devuelve:\n",
    "    - pandas.DataFrame: DataFrame con la columna de análisis de sentimiento agregada.\n",
    "    \"\"\"\n",
    "    def get_sentiment(text):\n",
    "        if pd.isna(text):\n",
    "            return 1  # Neutral si la reseña está ausente\n",
    "        analysis = TextBlob(text)\n",
    "        if analysis.sentiment.polarity > 0.1:  # Umbral para positivo\n",
    "            return 2  # Positivo\n",
    "        elif analysis.sentiment.polarity < -0.1:  # Umbral para negativo\n",
    "            return 0  # Negativo\n",
    "        else:\n",
    "            return 1  # Neutral\n",
    "\n",
    "    # Aplicar la función de análisis de sentimiento a la columna de reseñas\n",
    "    df['sentiment_analysis'] = df['review'].apply(get_sentiment)\n",
    " \n",
    "    return df\n",
    "\n",
    "# Aplicar la función\n",
    "sentiment_function = analyze_sentiment(reviews_exploded)\n",
    "sentiment_function = sentiment_function[['user_id','item_id','sentiment_analysis','recommend']]\n",
    "\n",
    "# JOIN con el DF steam_games\n",
    "# Hacer un join  con las df raw_utiems y raw_games, pero solo conserva las columnas estrictamente necesarias\n",
    "game_reviews = pd.merge(sentiment_function[['user_id','item_id','sentiment_analysis','recommend']], raw_games_csv_clean[['id', 'release_date', 'title', 'developer']], left_on='item_id', right_on='id', how='inner')\n",
    "# Convertir 'release_date' a formato de fecha (datetime)\n",
    "game_reviews['release_date'] = pd.to_datetime(game_reviews['release_date'],errors='coerce')\n",
    "# Crea una nueva columna que solo contiene el año\n",
    "game_reviews['release_year'] = game_reviews['release_date'].dt.year\n",
    "\n",
    "###-----Crea el CSV que servirá para la función 3: UsersRecommend----\n",
    "#game_reviews.to_csv(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\F03_UsersRecommend.csv', index=False)\n",
    "\n",
    "###-----Crea el parquet que servirá para la función 3: UsersRecommend----\n",
    "game_reviews.to_parquet(r'G:\\My Drive\\HENRY\\PROYECTO_INDIVIDUAL_I\\PI01-ABRIL2024\\DATASET\\F03_UsersRecommend.parquet', index=False)\n",
    "#print(game_reviews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
